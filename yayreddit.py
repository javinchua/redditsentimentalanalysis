# -*- coding: utf-8 -*-
"""yayreddit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RkSq38E3yIMjKjAIGUneYRPVXs-jB0Nr

submissions: the initial list of submissions

submission_df: dataframe that contains: 1. submission titles , 2. submission id

mask: True/False of whether title contains searched keyword

ID: ID of particular post (need to give the index of the post)

submission: a particular submission I decide to look at after determining the submission ID

comment_df: dataframe that contains all top level comments (1 column x ??? rows)

sia -  SentimentIntensityAnalyzer

result_comment: list of results of the comments - where neg neu and pos score 
is stated per comment

result_comment_df: a dataframe containing the above

comment_sentiment_df: a dataframe concatenating comment_df and result_comment_df - and later on a "label" is added

result_title : list of results of the titles - where neg neu and pos score is 
stated per comment (note is all top posts, for now cant filter yet)

result_title_df: a dataframe containing the above

title_sentiment_df: a dataframe concatenating submission_df and result_title_df - and later on a "label" is added

## Loading Necessary Libraries
"""

!pip install praw

!pip install emoji

!pip install praw
!pip install emoji

import pandas as pd
import numpy as np

import praw

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize, RegexpTokenizer
from nltk.corpus import stopwords
import re

import matplotlib.pyplot as plt

import emoji
import en_core_web_sm
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer

nltk.download('vader_lexicon') 
nltk.download('punkt') 
nltk.download('stopwords')

pd.options.display.max_rows
pd.set_option('display.max_colwidth', -1)

"""## Extracting Titles from Reddit"""

r = praw.Reddit(user_agent='Comfortable_Sun9200',
                client_id='4lLNCudJgjVjgep1CEIrmA',
                client_secret='tU9-IdncE28jIKOmjF-qzrTQdOW3pA',
                check_for_async=False)
# cookies123

#ask user what subreddit they want to see
while(True):
  try:
    subreddit_name = input('Enter the subreddit you would like to see: ')
    subreddit= r.subreddit(subreddit_name)
    #ask user what timeperiod of the top posts they would like to see
    time_period = input('Choose from the following timeframes - "all", "day", "hour", "month", "week", "year" : ')
    #submissions is a list of the top posts in a subreddit
    submissions = [*subreddit.top(time_period, limit=None)]
    number_of_posts = len(submissions)
    print("There are " + str(number_of_posts) + " posts")
    break
  # except statement will activate if subreddit is banned or not valid
  except:
    print("Please enter a valid subreddit/ timeframe")
    continue

title = [submission.title for submission in submissions]
id = [submission.id for submission in submissions]
submission_df = pd.DataFrame({
    "title": title,
    "id": id
})

submission_df

"""### Sentiment Analysis of All Top Posts

#### Text Preprocessing of Titles
"""

def clean_emoji(comment):
  comment_emojiless = emoji.get_emoji_regexp().sub(u'', comment)
  return comment_emojiless

def remove_websites(comment):
  comments_without_website = re.sub(r'http\S+', '', comment)
  return comments_without_website

def tokenize_comment(comment):
  tokenizer = RegexpTokenizer('\w+|\$[\d\.]+|http\S+')
  tokenized_comment = tokenizer.tokenize(comment)
  return tokenized_comment

def lower_comment(comment):
  lowered_comment = [word.lower() for word in comment]
  return lowered_comment

nlp = en_core_web_sm.load()
all_stopwords = nlp.Defaults.stop_words

def remove_stopwords(comment):
  text = comment
  comment_without_sw = [word for word in text if not word in all_stopwords]
  return comment_without_sw

lemmatizer = WordNetLemmatizer()

def lemmatize_comment(comment):
  lemmatized_comment = ([lemmatizer.lemmatize(w) for w in comment])
  return lemmatized_comment

# create new column with cleaned title
submission_df["cleaned_title"] = submission_df["title"]
submission_df["cleaned_title"] = submission_df["cleaned_title"].apply(lambda x : clean_emoji(x))
submission_df["cleaned_title"] = submission_df["cleaned_title"].apply(lambda x : remove_websites(x))
submission_df["cleaned_title"] = submission_df["cleaned_title"].apply(lambda x : tokenize_comment(x))
submission_df["cleaned_title"] = submission_df["cleaned_title"].apply(lambda x : lower_comment(x))
submission_df["cleaned_title"] = submission_df["cleaned_title"].apply(lambda x : remove_stopwords(x))
submission_df["cleaned_title"] = submission_df["cleaned_title"].apply(lambda x : lemmatize_comment(x))
submission_df["cleaned_title"] = submission_df["cleaned_title"].apply(lambda x : ' '.join(x))

submission_df

"""#### Labelling Sentiment of Titles"""

sia = SentimentIntensityAnalyzer()
result_title = [*submission_df['cleaned_title'].apply(sia.polarity_scores)]
# pprint(result_title[:3])

result_title_df = pd.DataFrame.from_records(result_title)
title_sentiment_df = pd.concat([submission_df, result_title_df], axis=1, join='inner')
# title_sentiment_df.head()

THRESHOLD = 0.2

conditions = [
    (title_sentiment_df['compound'] <= -THRESHOLD),
    (title_sentiment_df['compound'] > -THRESHOLD) & (title_sentiment_df['compound'] < THRESHOLD),
    (title_sentiment_df['compound'] >= THRESHOLD),
    ]

values = ["neg", "neu", "pos"]
title_sentiment_df['label'] = np.select(conditions, values)

title_sentiment_df.head()

display(title_sentiment_df.label.value_counts())
title_sentiment_df.label.value_counts().plot(kind='pie',  title = 'Sentiment Analysis of Posts', autopct='%1.1f%%', fontsize=17)

"""### Filtering Posts via Keyword"""

while(True):
  keyword = input('Enter keyword to filter titles: ')
  mask = title_sentiment_df['title'].str.contains(keyword, case = False, regex=False) # i dont understand the difference between treating 'china' as a regular expression vs as a string 
  #need to edit when figure out how to check if no title contains the keyword
  if mask.bool == True:
    continue
  else:
    break
# use this to search for any keyword you want to find within the titles of the top posts
display(title_sentiment_df[mask]) # later on i need to make sure this shows all articles in case user searches a common word
display(title_sentiment_df[mask].label.value_counts())
title_sentiment_df[mask].label.value_counts().plot(kind='pie',  title = 'Sentiment Analysis of Posts containing ' + keyword ,autopct='%1.1f%%', fontsize=17)

"""### Sentiment Analysis of Comments from Particular Post

"""

while True:
  post_index = input('Enter the index of the post to analyse: ')
  try:
      post_index = int(post_index)
  except ValueError:
      print("Enter a Valid Integer")
      continue
  if not (post_index >= 0 and post_index < number_of_posts):
    print("Please enter a valid post index that is between 0 and " + str((number_of_posts)-1))
  else:
    break
ID = submission_df["id"][post_index]

submission = r.submission(id="pw6yml") # what's the diff between submission and submissions? what am i referring back to when i state r.submission here?

comments_all = [] # store all comments in a list
submission.comments.replace_more(limit=None)
for comments in submission.comments.list():
  comments_all.append(comments.body)

print(comments_all, '\n')
print('Total Comments Scraped = ', (len(comments_all)))

"""now i want to clean the comments_all individually and then tabulate the number of positive vs negative comments - note i do not want to combine the comments together!"""

comments_cleaned = []

for i in comments_all:
  i = clean_emoji(i)
  i = remove_websites(i)
  i = tokenize_comment(i)
  i = lower_comment(i)
  i = remove_stopwords(i)
  i = lemmatize_comment(i)
  comments_cleaned.append(i)

for i in range(len(comments_cleaned)):
  comments_cleaned[i] = ' '.join(comments_cleaned[i])

""""I wonder what steps the Chinese government will take to ensure the illegal ivory trade doesn't grow following this policy change"

['wonder step chinese government ensure illegal ivory trade doesn t grow following policy change'
"""

for i in comments_cleaned:
  if i == "removed":
    comments_cleaned.remove(i)

for i in comments_cleaned:
  if i == "deleted":
    comments_cleaned.remove(i)

comments_cleaned

cleaned_comments_df = pd.DataFrame(comments_cleaned)
cleaned_comments_df.columns = ['comment']

result_comment = [*cleaned_comments_df["comment"].apply(sia.polarity_scores)] # what is the * for?
# gives a score to every commment in the comment_df DataFrame
# print(result_comment)

result_comment_df = pd.DataFrame.from_records(result_comment)
comment_sentiment_df = pd.concat([cleaned_comments_df, result_comment_df], axis=1, join='inner')
comment_sentiment_df.head()
# comment_sentiment_df is a dataframe which combines comment_df and sentiment_df

THRESHOLD = 0.2 # this 0.2 threshold can be amended based on how sensitive you want the categorising of pos/neg comments to be

conditions = [
    (comment_sentiment_df['compound'] <= -THRESHOLD),
    (comment_sentiment_df['compound'] > -THRESHOLD) & (comment_sentiment_df['compound'] < THRESHOLD),
    (comment_sentiment_df['compound'] >= THRESHOLD),
    ]

values = ["neg", "neu", "pos"]
comment_sentiment_df['label'] = np.select(conditions, values)

comment_sentiment_df.head()

display(comment_sentiment_df.label.value_counts())
comment_sentiment_df.label.value_counts().plot(kind='pie', title = 'Sentiment Analysis of Comment Section', autopct='%1.1f%%', fontsize=17)

"""### Ignore past this"""

# comment_df1 = pd.DataFrame(comments_cleaned)
# comment_df1

from praw.models import MoreComments

comment_df = pd.DataFrame()
for top_level_comment in submission.comments:
    if isinstance(top_level_comment, MoreComments):
        continue # ignores "MoreComments"

    comment_df = comment_df.append({"comment": top_level_comment.body}, ignore_index=True)
    # creates a df to store comments (note that only top level comments are stored, as there would be too many comments otherwise)

# print(comment_df.iloc[1])

result_comment = [*comment_df['comment'].apply(sia.polarity_scores)] # what is the * for?
# gives a score to every commment in the comment_df DataFrame
# print(result_comment)

result_comment_df = pd.DataFrame.from_records(result_comment)
comment_sentiment_df = pd.concat([comment_df, result_comment_df], axis=1, join='inner')
comment_sentiment_df.head()
# comment_sentiment_df is a dataframe which combines comment_df and sentiment_df

THRESHOLD = 0.2 # this 0.2 threshold can be amended based on how sensitive you want the categorising of pos/neg comments to be

conditions = [
    (comment_sentiment_df['compound'] <= -THRESHOLD),
    (comment_sentiment_df['compound'] > -THRESHOLD) & (comment_sentiment_df['compound'] < THRESHOLD),
    (comment_sentiment_df['compound'] >= THRESHOLD),
    ]

values = ["neg", "neu", "pos"]
comment_sentiment_df['label'] = np.select(conditions, values)

comment_sentiment_df.head()

display(comment_sentiment_df.label.value_counts())
comment_sentiment_df.label.value_counts().plot(kind='pie', autopct='%1.1f%%', fontsize=17)

# print(submission.comments.list())
# this allows me to find all the comment IDs of the comments in the particular post - incase i need this line next time

"""PIE CHART!!!!"""